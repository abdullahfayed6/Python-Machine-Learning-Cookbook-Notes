{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["BAKJgZaaTjSe","laabKjMXZbsl","V584BxTQ7RKy","FcTTRfs68ymg","Mr2FJf4__Ph-"],"authorship_tag":"ABX9TyNB3ipUgf51uHJpuZwq4Q07"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA, KernelPCA\n","from sklearn.decomposition import NMF\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.decomposition import TruncatedSVD\n","from scipy.sparse import csr_matrix\n","from sklearn import datasets\n","import numpy as np"],"metadata":{"id":"-jHLd3SgTvfG","executionInfo":{"status":"ok","timestamp":1751793453671,"user_tz":-180,"elapsed":45,"user":{"displayName":"Abdullah Fayed","userId":"07741069294113967274"}}},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["# **Reducing Features Using Principal Components (PCA)**"],"metadata":{"id":"BAKJgZaaTjSe"}},{"cell_type":"code","source":["# Load the data\n","digits = datasets.load_digits()\n","\n","# Standardize the feature matrix\n","features = StandardScaler().fit_transform(digits.data)\n","\n","# Create a PCA that will retain 99% of variance\n","pca = PCA(n_components=0.99, whiten=True)\n","\n","# Conduct PCA\n","features_pca = pca.fit_transform(features)\n","\n","# Show results\n","print(\"Original number of features:\", features.shape[1])\n","print(\"Reduced number of features:\", features_pca.shape[1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qDy-ujrJTjDm","executionInfo":{"status":"ok","timestamp":1751714480356,"user_tz":-180,"elapsed":178,"user":{"displayName":"Abdullah Fayed","userId":"07741069294113967274"}},"outputId":"2c9fac2c-2703-4dd9-8321-ab7fbcaed099"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original number of features: 64\n","Reduced number of features: 54\n"]}]},{"cell_type":"markdown","source":["**The output of solution shows that PCA let us reduce our dimensionality by 10\n","features while still retaining 99% of the information (variance) in the feature matrix.**"],"metadata":{"id":"_BGRHSDfVBrP"}},{"cell_type":"markdown","source":["# **Reducing Features When Data Is Linearly Inseparable**"],"metadata":{"id":"laabKjMXZbsl"}},{"cell_type":"code","source":["# Create linearly inseparable data\n","features, _ = datasets.make_circles(n_samples=1000, random_state=1, noise=0.1, factor=0.1)\n","\n","# Apply kernal PCA with radius basis function (RBF) kernel\n","kpca = KernelPCA(kernel=\"rbf\", gamma=15, n_components=1)\n","features_kpca = kpca.fit_transform(features)\n","\n","print(\"Original number of features:\", features.shape[1])\n","print(\"Reduced number of features:\", features_kpca.shape[1])"],"metadata":{"id":"LpzKF5Z_UO3n","executionInfo":{"status":"ok","timestamp":1751791755127,"user_tz":-180,"elapsed":142,"user":{"displayName":"Abdullah Fayed","userId":"07741069294113967274"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a53ded8b-9aed-4e92-91d9-e8160725a2ef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original number of features: 2\n","Reduced number of features: 1\n"]}]},{"cell_type":"markdown","source":["# **Reducing Features by Maximizing Class Separability**"],"metadata":{"id":"V584BxTQ7RKy"}},{"cell_type":"code","source":["# Load Iris flower dataset:\n","iris = datasets.load_iris()\n","features = iris.data\n","target = iris.target\n","\n","# Create and run an LDA, then use it to transform the features\n","lda = LinearDiscriminantAnalysis(n_components=1)\n","features_lda = lda.fit(features, target).transform(features)\n","\n","# Print the number of features\n","print(\"Original number of features:\", features.shape[1])\n","print(\"Reduced number of features:\", features_lda.shape[1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5SWlaKL67S7b","executionInfo":{"status":"ok","timestamp":1751791842064,"user_tz":-180,"elapsed":43,"user":{"displayName":"Abdullah Fayed","userId":"07741069294113967274"}},"outputId":"e4421541-dcf1-4756-85f8-33098af99f1c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original number of features: 4\n","Reduced number of features: 1\n"]}]},{"cell_type":"code","source":["lda.explained_variance_ratio_"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gt7HemZt7m3q","executionInfo":{"status":"ok","timestamp":1751791859579,"user_tz":-180,"elapsed":11,"user":{"displayName":"Abdullah Fayed","userId":"07741069294113967274"}},"outputId":"34289065-11b4-49c1-806c-a8e631cc9a95"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.9912126])"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["**Specifically, we can run LinearDiscriminantAnalysis with n_components set to\n","None to return the ratio of variance explained by every component feature, then calculate\n","how many components are required to get above some threshold of variance\n","explained (often 0.95 or 0.99):**"],"metadata":{"id":"BmpSADcX8ebI"}},{"cell_type":"code","source":["# Create and run LDA\n","lda = LinearDiscriminantAnalysis(n_components=None)\n","features_lda = lda.fit(features, target)\n","\n","# Create array of explained variance ratios\n","lda_var_ratios = lda.explained_variance_ratio_\n","\n","# Create function\n","def select_n_components(var_ratio, goal_var: float) -> int:\n","\n","    # Set initial variance explained so far\n","    total_variance = 0.0\n","\n","    # Set initial number of features\n","    n_components = 0\n","\n","    # For the explained variance of each feature:\n","    for explained_variance in var_ratio:\n","\n","        # Add the explained variance to the total\n","        total_variance += explained_variance\n","\n","        # Add one to the number of components\n","        n_components += 1\n","\n","        # If we reach our goal level of explained variance\n","        if total_variance >= goal_var:\n","\n","          # End the loop\n","          break\n","\n","    # Return the number of components\n","    return n_components\n","\n","# Run function\n","select_n_components(lda_var_ratios, 0.95)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wf6aeRcm8FfH","executionInfo":{"status":"ok","timestamp":1751792156038,"user_tz":-180,"elapsed":37,"user":{"displayName":"Abdullah Fayed","userId":"07741069294113967274"}},"outputId":"aa9b4e5c-0113-4c8e-8351-d20dfa0844e6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["# **Reducing Features Using Matrix Factorization**"],"metadata":{"id":"FcTTRfs68ymg"}},{"cell_type":"code","source":["# Load the data\n","digits = datasets.load_digits()\n","\n","# Load feature matrix\n","features = digits.data\n","\n","# Create, fit, and apply NMF\n","nmf = NMF(n_components=10, random_state=1)\n","features_nmf = nmf.fit_transform(features)\n","\n","# Show results\n","print(\"Original number of features:\", features.shape[1])\n","print(\"Reduced number of features:\", features_nmf.shape[1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5eRo4NGH82dJ","executionInfo":{"status":"ok","timestamp":1751792676472,"user_tz":-180,"elapsed":1106,"user":{"displayName":"Abdullah Fayed","userId":"07741069294113967274"}},"outputId":"f2c986af-155a-4ea1-9971-7c10e105fdb9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original number of features: 64\n","Reduced number of features: 10\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["# **Reducing Features on Sparse**"],"metadata":{"id":"Mr2FJf4__Ph-"}},{"cell_type":"code","source":["# Load the data\n","digits = datasets.load_digits()\n","\n","# Standardize feature matrix\n","features = StandardScaler().fit_transform(digits.data)\n","\n","# Make sparse matrix\n","features_sparse = csr_matrix(features)\n","\n","# Create a TSVD\n","tsvd = TruncatedSVD(n_components=10)\n","\n","# Conduct TSVD on sparse matrix\n","features_sparse_tsvd = tsvd.fit(features_sparse).transform(features_sparse)\n","\n","# Show results\n","print(\"Original number of features:\", features_sparse.shape[1])\n","print(\"Reduced number of features:\", features_sparse_tsvd.shape[1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QFF94rvc_VVb","executionInfo":{"status":"ok","timestamp":1751792888105,"user_tz":-180,"elapsed":84,"user":{"displayName":"Abdullah Fayed","userId":"07741069294113967274"}},"outputId":"b3a769f7-fd6e-4c83-856c-74f864f36d9f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original number of features: 64\n","Reduced number of features: 10\n"]}]},{"cell_type":"code","source":["# Sum of first three components' explained variance ratios\n","tsvd.explained_variance_ratio_[0:3].sum()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"doYx2kRU_u0P","executionInfo":{"status":"ok","timestamp":1751792939899,"user_tz":-180,"elapsed":14,"user":{"displayName":"Abdullah Fayed","userId":"07741069294113967274"}},"outputId":"18981c61-b095-434c-a3d4-c08bf389bb0b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["np.float64(0.3003938538720454)"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["**We can automate the process by creating a function that runs TSVD with n_compo\n","nents set to one less than the number of original features and then calculate the number\n","of components that explain a desired amount of the original data’s variance:**"],"metadata":{"id":"uGweqFUc_zUC"}},{"cell_type":"code","source":["# Create and run an TSVD with one less than number of features\n","tsvd = TruncatedSVD(n_components=features_sparse.shape[1]-1)\n","features_tsvd = tsvd.fit(features)\n","\n","# List of explained variances\n","tsvd_var_ratios = tsvd.explained_variance_ratio_\n","\n","# Create a function\n","def select_n_components(var_ratio, goal_var):\n","\n","    # Set initial variance explained so far\n","    total_variance = 0.0\n","\n","    # Set initial number of features\n","    n_components = 0\n","\n","    # For the explained variance of each feature:\n","    for explained_variance in var_ratio:\n","\n","        # Add the explained variance to the total\n","        total_variance += explained_variance\n","\n","        # Add one to the number of components\n","        n_components += 1\n","\n","        # If we reach our goal level of explained variance\n","        if total_variance >= goal_var:\n","\n","            # End the loop\n","            break\n","\n","    return n_components\n","\n","# Run function\n","select_n_components(tsvd_var_ratios, 0.95)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f_sopU30_4Fc","executionInfo":{"status":"ok","timestamp":1751793072483,"user_tz":-180,"elapsed":258,"user":{"displayName":"Abdullah Fayed","userId":"07741069294113967274"}},"outputId":"464bde60-ef75-4fa7-ff16-013af7cc5a7b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["40"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","source":["# Important Notes\n"],"metadata":{"id":"yTSLhnvlA4Sq"}},{"cell_type":"markdown","source":["| Technique      | Type      | Works With        | Uses Labels? | Best For                                      | Notes                              |\n","| -------------- | --------- | ----------------- | ------------ | --------------------------------------------- | ---------------------------------- |\n","| **PCA**        | Linear    | Any numeric data  | ❌ No         | Reducing features while preserving variance   | Keeps most variance                |\n","| **Kernel PCA** | Nonlinear | Complex patterns  | ❌ No         | Nonlinear data (e.g., circular shapes)        | Good for curved data               |\n","| **LDA**        | Linear    | Classification    | ✅ Yes        | Supervised classification problems            | Maximizes class separation         |\n","| **NMF**        | Linear    | Non-negative data | ❌ No         | Topic modeling, interpretability (e.g., text) | Used in NLP, interpretable results |\n","| **TSVD**       | Linear    | Sparse data       | ❌ No         | Text data, large sparse datasets              | Efficient on big text-like data    |\n"],"metadata":{"id":"5kMv5pmmShOf"}},{"cell_type":"markdown","source":["* Use **PCA** if your data is numeric and you\n","want a quick, general-purpose dimensionality reduction.\n","\n","* Use **Kernel PCA** if PCA doesn’t work well and the data seems nonlinear (e.g., spiral or circular shapes).\n","\n","* Use **LDA** if you're working on a classification task and want to maximize separation between classes.\n","\n","* Use **NMF** if your data is non-negative, and you want meaningful components (good for NLP topics).\n","\n","* Use **TSVD** if you're dealing with sparse, high-dimensional data (like bag-of-words or TF-IDF in text)."],"metadata":{"id":"TYInlfkoAnFm"}}]}